{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/baldeeb/Code/pytorch-NOCS')\n",
    "\n",
    "from models.nocs import get_nocs_resnet50_fpn\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_Weights\n",
    "from habitat_datagen_util.utils.dataset import HabitatDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from habitat_datagen_util.utils.collate_tools import collate_fn\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "\n",
    "DATA_DIR = \"/home/baldeeb/Code/pytorch-NOCS/data/habitat/train\"  # larger dataset\n",
    "device='cuda:1'\n",
    "\n",
    "model = get_nocs_resnet50_fpn(\n",
    "                maskrcnn_weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT,\n",
    "                nocs_num_bins=32,\n",
    "                nocs_loss_mode = 'classification',\n",
    "                multiheaded_nocs = True,\n",
    "\n",
    "                )\n",
    "model.to(device).train()\n",
    "habitatdata = HabitatDataset(DATA_DIR)\n",
    "dataloader = DataLoader(habitatdata, \n",
    "                        batch_size=2, \n",
    "                        shuffle=True, \n",
    "                        collate_fn=collate_fn)\n",
    "optimizer = Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def targets2device(targets, device):\n",
    "    for i in range(len(targets)): \n",
    "        for k in ['masks', 'labels', 'boxes']: \n",
    "            targets[i][k] = targets[i][k].to(device)\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baldeeb/miniconda3/envs/torch-nocs/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.54 GiB (GPU 0; 23.66 GiB total capacity; 20.43 GiB already allocated; 923.00 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m targets \u001b[39m=\u001b[39m targets2device(targets, device)\n\u001b[0;32m----> 5\u001b[0m losses \u001b[39m=\u001b[39m model(images, targets)\n\u001b[1;32m      6\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(losses\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m      7\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nocs/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Code/pytorch-NOCS/models/nocs.py:135\u001b[0m, in \u001b[0;36mNOCS.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, images, targets\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 135\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(images, targets)\n\u001b[1;32m    136\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroi_heads\u001b[39m.\u001b[39mcache_results \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m    137\u001b[0m         original_sizes \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor([img\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:] \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m images])\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nocs/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py:105\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    103\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n\u001b[1;32m    104\u001b[0m proposals, proposal_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrpn(images, features, targets)\n\u001b[0;32m--> 105\u001b[0m detections, detector_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroi_heads(features, proposals, images\u001b[39m.\u001b[39;49mimage_sizes, targets)\n\u001b[1;32m    106\u001b[0m detections \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mpostprocess(detections, images\u001b[39m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[39m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m    108\u001b[0m losses \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-nocs/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Code/pytorch-NOCS/models/nocs_roi_heads.py:120\u001b[0m, in \u001b[0;36mRoIHeadsWithNocs.forward\u001b[0;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39m    features (List[Tensor]): features produced by the backbone \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m    targets (List[Dict]): \u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_training_mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmultiview\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmultiview_consistency_loss(features, proposals, image_shapes, targets)\n\u001b[1;32m    122\u001b[0m \u001b[39mif\u001b[39;00m targets \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_target_contents(targets)\n\u001b[1;32m    124\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n",
      "File \u001b[0;32m~/Code/pytorch-NOCS/models/nocs_roi_heads.py:309\u001b[0m, in \u001b[0;36mRoIHeadsWithNocs.multiview_consistency_loss\u001b[0;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[1;32m    306\u001b[0m k\u001b[39m.\u001b[39mappend(t[\u001b[39m'\u001b[39m\u001b[39mintrinsics\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    308\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m r[\u001b[39m'\u001b[39m\u001b[39mnocs\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m--> 309\u001b[0m     r[\u001b[39m'\u001b[39m\u001b[39mnocs\u001b[39m\u001b[39m'\u001b[39m][key] \u001b[39m=\u001b[39m paste_in_image(r[\u001b[39m'\u001b[39;49m\u001b[39mnocs\u001b[39;49m\u001b[39m'\u001b[39;49m][key],\n\u001b[1;32m    310\u001b[0m                                     r[\u001b[39m'\u001b[39;49m\u001b[39mboxes\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    311\u001b[0m                                     t[\u001b[39m'\u001b[39;49m\u001b[39mdepth\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mshape[\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m],\n\u001b[1;32m    312\u001b[0m                                     t[\u001b[39m'\u001b[39;49m\u001b[39mdepth\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mshape[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m],)\n\u001b[1;32m    313\u001b[0m \u001b[39m# n.append(torch.stack(list(r['nocs'].values()), dim=1))\u001b[39;00m\n\u001b[1;32m    315\u001b[0m ni \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(\u001b[39mlist\u001b[39m(r[\u001b[39m'\u001b[39m\u001b[39mnocs\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues()), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Code/pytorch-NOCS/models/nocs_util.py:108\u001b[0m, in \u001b[0;36mpaste_in_image\u001b[0;34m(data, box, im_h, im_w)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39m# Resize mask\u001b[39;00m\n\u001b[1;32m    104\u001b[0m resized \u001b[39m=\u001b[39m [F\u001b[39m.\u001b[39minterpolate(data[i:i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m], size\u001b[39m=\u001b[39m(h[i], w[i]), \n\u001b[1;32m    105\u001b[0m                          mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m\"\u001b[39m, align_corners\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    106\u001b[0m            \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(B)]\n\u001b[0;32m--> 108\u001b[0m images \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros((B, C, im_h, im_w),\n\u001b[1;32m    109\u001b[0m                       dtype\u001b[39m=\u001b[39;49mresized[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mdtype,\n\u001b[1;32m    110\u001b[0m                       device\u001b[39m=\u001b[39;49mresized[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    111\u001b[0m x0 \u001b[39m=\u001b[39m box[:, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mclamp(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39mim_w)\u001b[39m.\u001b[39mlong()\n\u001b[1;32m    112\u001b[0m x1 \u001b[39m=\u001b[39m (box[:, \u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mclamp(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39mim_w)\u001b[39m.\u001b[39mlong()\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.54 GiB (GPU 0; 23.66 GiB total capacity; 20.43 GiB already allocated; 923.00 MiB free; 21.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model.roi_heads.training_mode('multiview')\n",
    "for itr, (images, targets) in enumerate(dataloader):\n",
    "    images = images.to(device)\n",
    "    targets = targets2device(targets, device)\n",
    "    losses = model(images, targets)\n",
    "    loss = sum(losses.values())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
